from sklearn.cluster import KMeans, DBSCAN
import joblib
import streamlit as st
import os
import numpy as np
import pandas as pd
import pickle
import seaborn as sns
import random
import struct
import altair
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay
from PIL import Image
from collections import Counter
import mlflow
import mlflow.sklearn
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report, precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV
import kagglehub



st.set_page_config(page_title="Ph√¢n lo·∫°i ·∫£nh", layout="wide")

# Streamlit app
st.title("MNIST Classification with Streamlit & MLFlow")

@st.cache_data  # L∆∞u cache ƒë·ªÉ tr√°nh load l·∫°i d·ªØ li·ªáu m·ªói l·∫ßn ch·∫°y l·∫°i Streamlit
def get_sampled_pixels(images, sample_size=100_000):
    return np.random.choice(images.flatten(), sample_size, replace=False)

@st.cache_data  # Cache danh s√°ch ·∫£nh ng·∫´u nhi√™n
def get_random_indices(num_images, total_images):
    return np.random.randint(0, total_images, size=num_images)

# # C·∫•u h√¨nh Streamlit
# def config_page():
#     st.set_page_config(page_title="Ph√¢n lo·∫°i ·∫£nh", layout="wide", initial_sidebar_state="expanded")

# config_page()

# st.set_page_config(page_title="Ph√¢n lo·∫°i ·∫£nh", layout="wide", initial_sidebar_state="expanded")
# ƒê·ªãnh nghƒ©a h√†m ƒë·ªÉ ƒë·ªçc file .idx
def load_mnist_images(filename):
    with open(filename, 'rb') as f:
        magic, num, rows, cols = struct.unpack('>IIII', f.read(16))
        images = np.fromfile(f, dtype=np.uint8).reshape(num, rows, cols)
    return images

def load_mnist_labels(filename):
    with open(filename, 'rb') as f:
        magic, num = struct.unpack('>II', f.read(8))
        labels = np.fromfile(f, dtype=np.uint8)
    return labels

# ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n ƒë·∫øn c√°c file MNIST
# Download latest version
dataset_path = kagglehub.dataset_download("hojjatk/mnist-dataset")
# dataset_path = os.path.dirname(os.path.abspath(__file__))
train_images_path = os.path.join(dataset_path, "train-images.idx3-ubyte")
train_labels_path = os.path.join(dataset_path, "train-labels.idx1-ubyte")
test_images_path = os.path.join(dataset_path, "t10k-images.idx3-ubyte")
test_labels_path = os.path.join(dataset_path, "t10k-labels.idx1-ubyte")

# T·∫£i d·ªØ li·ªáu
train_images = load_mnist_images(train_images_path)
train_labels = load_mnist_labels(train_labels_path)
test_images = load_mnist_images(test_images_path)
test_labels = load_mnist_labels(test_labels_path)

st.write(f"S·ªë l∆∞·ª£ng ·∫£nh trong t·∫≠p train: {len(train_images)}")
st.write(f"S·ªë l∆∞·ª£ng ·∫£nh trong t·∫≠p train: {len(test_images)}")
st.subheader("Ch·ªçn ng·∫´u nhi√™n 10 ·∫£nh t·ª´ t·∫≠p hu·∫•n luy·ªán ƒë·ªÉ hi·ªÉn th·ªã")
num_images = 10
random_indices = random.sample(range(len(train_images)), num_images)
fig, axes = plt.subplots(1, num_images, figsize=(15, 5))

for ax, idx in zip(axes, random_indices):
        ax.imshow(train_images[idx], cmap='gray')
        ax.axis("off")
        ax.set_title(f"Label: {train_labels[idx]}")

st.pyplot(fig)

# Flatten the images
X_train = train_images.reshape(-1, 28 * 28)
X_test = test_images.reshape(-1, 28 * 28)
y_train = train_labels
y_test = test_labels

# Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi nh√£n d·ªØ li·ªáu
fig, ax = plt.subplots(figsize=(6, 4))
sns.barplot(x=list(Counter(y_train).keys()), y=list(Counter(y_train).values()), palette="Blues", ax=ax)
ax.set_title("Ph√¢n ph·ªëi nh√£n trong t·∫≠p hu·∫•n luy·ªán")
ax.set_xlabel("Nh√£n")
ax.set_ylabel("S·ªë l∆∞·ª£ng")
st.pyplot(fig)

# Normalize the data
X_train = X_train.astype("float32") / 255.0
X_test = X_test.astype("float32") / 255.0

# T·∫°o b·ªô d·ªØ li·ªáu
train_data = (train_images, train_labels)
test_data = (test_images, test_labels)

# # Ki·ªÉm tra gi√° tr·ªã null ho·∫∑c NaN trong d·ªØ li·ªáu
# na_counts = X.isna().sum()
# st.write("S·ªë l∆∞·ª£ng gi√° tr·ªã null ho·∫∑c NaN trong d·ªØ li·ªáu:", na_counts)

# # Ki·ªÉm tra gi√° tr·ªã v√¥ h·∫°n trong d·ªØ li·ªáu
# inf_counts = X.isinf().sum()
# st.write("S·ªë l∆∞·ª£ng gi√° tr·ªã v√¥ h·∫°n trong d·ªØ li·ªáu:", inf_counts)

# # ƒê·∫øm s·ªë l∆∞·ª£ng c·ªßa m·ªói gi√° tr·ªã trong d·ªØ li·ªáu
# value_counts = X.apply(lambda x: x.value_counts())
# st.write("S·ªë l∆∞·ª£ng c·ªßa m·ªói gi√° tr·ªã trong d·ªØ li·ªáu:", value_counts)

# # T·∫°o ph·∫ßn t√πy ch·ªçn chia d·ªØ li·ªáu train
# st.subheader("T√πy ch·ªçn chia d·ªØ li·ªáu train")
# train_ratio = st.slider("T·ª∑ l·ªá d·ªØ li·ªáu train (%)", min_value=10, max_value=90, value=70, step=1)
# test_ratio = 100 - train_ratio
# a = 100 - train_ratio

# # Chia t√°ch d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra
# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio/100, random_state=42)

# # T·∫°o ph·∫ßn t√πy ch·ªçn chia d·ªØ li·ªáu test th√†nh validation v√† test
# st.subheader("T√πy ch·ªçn chia d·ªØ li·ªáu test th√†nh validation v√† test")
# val_ratio = st.slider("T·ª∑ l·ªá d·ªØ li·ªáu validation (%)", min_value=0, max_value=a, value=10, step=1)

# x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=(100-val_ratio)/100, random_state=42)

st.subheader("T√πy ch·ªçn chia d·ªØ li·ªáu train")
train_ratio = st.slider("T·ª∑ l·ªá d·ªØ li·ªáu train (%)", min_value=10, max_value=90, value=80, step=1)
test_ratio = 100 - train_ratio
a = 100 - train_ratio

# Chia t√°ch d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra
x_train, x_val_test, y_train, y_val_test = train_test_split(X_train, y_train, test_size=test_ratio/100, random_state=42)

# T·∫°o ph·∫ßn t√πy ch·ªçn chia d·ªØ li·ªáu test th√†nh validation v√† test
st.subheader("T√πy ch·ªçn chia d·ªØ li·ªáu test th√†nh validation v√† test")
val_ratio = st.slider("T·ª∑ l·ªá d·ªØ li·ªáu validation (%)", min_value=0, max_value=a, value=a, step=1)

x_val, x_test, y_val, y_test = train_test_split(x_val_test, y_val_test, test_size=(100-val_ratio)/100, random_state=42)

# In ra s·ªë l∆∞·ª£ng c·ªßa c√°c t·∫≠p train, test v√† val
st.subheader("S·ªë l∆∞·ª£ng c·ªßa c√°c t·∫≠p d·ªØ li·ªáu")
st.write("S·ªë l∆∞·ª£ng d·ªØ li·ªáu train: ", len(x_train))
st.write("S·ªë l∆∞·ª£ng d·ªØ li·ªáu validation: ", len(x_val))
st.write("S·ªë l∆∞·ª£ng d·ªØ li·ªáu test: ", len(x_test))

st.header("Ch·ªçn m√¥ h√¨nh & Hu·∫•n luy·ªán")

# Ch·ªçn m√¥ h√¨nh
model_choice = st.radio("Ch·ªçn m√¥ h√¨nh:", ["K-Means", "DBSCAN"])

if model_choice == "K-Means":
    st.markdown("""
    - **K-Means** l√† thu·∫≠t to√°n ph√¢n c·ª•m ph·ªï bi·∫øn, chia d·ªØ li·ªáu th√†nh K c·ª•m d·ª±a tr√™n kho·∫£ng c√°ch.
    - **Tham s·ªë c·∫ßn ch·ªçn:
        - S·ªë l∆∞·ª£ng c·ª•m (k).  
    """)
        
    n_clusters = st.slider("n_clusters", 2, 20, 10)
    model = KMeans(n_clusters=n_clusters, random_state=42)
    
elif model_choice == "DBSCAN":
    st.markdown("""
    - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** l√† thu·∫≠t to√°n ph√¢n c·ª•m d·ª±a tr√™n m·∫≠t ƒë·ªô.
    - **Tham s·ªë c·∫ßn ch·ªçn:**  
        - B√°n k√≠nh l√¢n c·∫≠n.  
        - S·ªë l∆∞·ª£ng ƒëi·ªÉm t·ªëi thi·ªÉu ƒë·ªÉ t·∫°o c·ª•m.  
    """)
    eps = st.slider("eps", 0.1, 10.0, 0.5)
    min_samples = st.slider("min_samples", 2, 20, 5)
    model = DBSCAN(eps=eps, min_samples=min_samples)

if st.button("Hu·∫•n luy·ªán m√¥ h√¨nh"):
    model.fit(X_train)
    labels = model.labels_
    st.success("‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng!")

    # L∆∞u m√¥ h√¨nh v√†o session_state d∆∞·ªõi d·∫°ng danh s√°ch n·∫øu ch∆∞a c√≥
    if "models" not in st.session_state:
        st.session_state["models"] = []

    model_name = model_choice.lower().replace(" ", "_")

    existing_model = next((item for item in st.session_state["models"] if item["name"] == model_name), None)
        
    if existing_model:
        count = 1
        new_model_name = f"{model_name}_{count}"
        while any(item["name"] == new_model_name for item in st.session_state["models"]):
            count += 1
            new_model_name = f"{model_name}_{count}"
        model_name = new_model_name
        st.warning(f"‚ö†Ô∏è M√¥ h√¨nh ƒë∆∞·ª£c l∆∞u v·ªõi t√™n l√†: {model_name}")

    st.session_state["models"].append({"name": model_name, "model": model})
    st.write(f"üîπ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v·ªõi t√™n: {model_name}")
    
        
    st.sidebar.subheader("Demo d·ª± ƒëo√°n ch·ªØ vi·∫øt tay")
    st.sidebar.write("Vui l√≤ng nh·∫≠p h√¨nh ·∫£nh ch·ªØ vi·∫øt tay ƒë·ªÉ d·ª± ƒëo√°n:")

    # T·∫°o ph·∫ßn nh·∫≠p h√¨nh ·∫£nh
    uploaded_file = st.sidebar.file_uploader("Ch·ªçn h√¨nh ·∫£nh", type=["png", "jpg", "jpeg"])

    # T·∫°o n√∫t ki·ªÉm tra
    if st.sidebar.button("Ki·ªÉm tra"):
        # X·ª≠ l√Ω h√¨nh ·∫£nh
        if uploaded_file is not None:
            image = Image.open(uploaded_file)
            image = image.resize((28, 28))
            image = image.convert('L')
            image = np.array(image)
            image = image.reshape(1, 784)

            # D·ª± ƒëo√°n ch·ªØ vi·∫øt tay
            prediction = model.predict(image)

            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            st.sidebar.write("K·∫øt qu·∫£ d·ª± ƒëo√°n:")
            st.sidebar.write("Ch·ªØ vi·∫øt tay:", prediction[0])
        else:
            st.sidebar.write("Vui l√≤ng nh·∫≠p h√¨nh ·∫£nh ch·ªØ vi·∫øt tay ƒë·ªÉ d·ª± ƒëo√°n:")


